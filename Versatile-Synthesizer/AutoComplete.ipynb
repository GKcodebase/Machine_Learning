{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK I. The Republic opens with a truly Greek scene--a festival in\n",
      "honour of the goddess Bendis which is held in the Piraeus; to this is\n",
      "added the promise of an equestrian torch-race in the evening. T\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'republic.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# replace '--' with a space ' '\n",
    "\tdoc = doc.replace('--', ' ')\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'i', 'the', 'republic', 'opens', 'with', 'a', 'truly', 'greek', 'scene', 'a', 'festival', 'in', 'honour', 'of', 'the', 'goddess', 'bendis', 'which', 'is', 'held', 'in', 'the', 'piraeus', 'to', 'this', 'is', 'added', 'the', 'promise', 'of', 'an', 'equestrian', 'torchrace', 'in', 'the', 'evening', 'the', 'whole', 'work', 'is', 'supposed', 'to', 'be', 'recited', 'by', 'socrates', 'on', 'the', 'day', 'after', 'the', 'festival', 'to', 'a', 'small', 'party', 'consisting', 'of', 'critias', 'timaeus', 'hermocrates', 'and', 'another', 'this', 'we', 'learn', 'from', 'the', 'first', 'words', 'of', 'the', 'timaeus', 'when', 'the', 'rhetorical', 'advantage', 'of', 'reciting', 'the', 'dialogue', 'has', 'been', 'gained', 'the', 'attention', 'is', 'not', 'distracted', 'by', 'any', 'reference', 'to', 'the', 'audience', 'nor', 'is', 'the', 'reader', 'further', 'reminded', 'of', 'the', 'extraordinary', 'length', 'of', 'the', 'narrative', 'of', 'the', 'numerous', 'company', 'three', 'only', 'take', 'any', 'serious', 'part', 'in', 'the', 'discussion', 'nor', 'are', 'we', 'informed', 'whether', 'in', 'the', 'evening', 'they', 'went', 'to', 'the', 'torchrace', 'or', 'talked', 'as', 'in', 'the', 'symposium', 'through', 'the', 'night', 'the', 'manner', 'in', 'which', 'the', 'conversation', 'has', 'arisen', 'is', 'described', 'as', 'follows', 'socrates', 'and', 'his', 'companion', 'glaucon', 'are', 'about', 'to', 'leave', 'the', 'festival', 'when', 'they', 'are', 'detained', 'by', 'a', 'message', 'from', 'polemarchus', 'who', 'speedily', 'appears', 'accompanied', 'by', 'adeimantus', 'the', 'brother', 'of', 'glaucon', 'and', 'with', 'playful', 'violence', 'compels', 'them', 'to', 'remain', 'promising', 'them', 'not', 'only', 'the', 'torchrace']\n",
      "Total Tokens: 210245\n",
      "Unique Tokens: 10270\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 210194\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save sequences to file\n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book i the republic opens with a truly greek scene a festival in honour of the goddess bendis which is held in the piraeus to this is added the promise of an equestrian torchrace in the evening the whole work is supposed to be recited by socrates on the day after\n"
     ]
    }
   ],
   "source": [
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Language Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[297, 15, 1, 306, 10270, 28, 7, 363, 225, 4604, 7, 1330, 6, 275, 2, 1, 3125, 4603, 11, 5, 850, 6, 1, 2711, 4, 31, 5, 1054, 1, 1765, 2, 49, 6301, 3706, 6, 1, 2710, 1, 147, 278, 5, 402, 4, 10, 6299, 23, 150, 57, 1, 357, 165]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10270, 28, 7, 363, 225, 4604, 7, 1330, 6, 275, 2, 1, 3125, 4603, 11, 5, 850, 6, 1, 2711, 4, 31, 5, 1054, 1, 1765, 2, 49, 6301, 3706, 6, 1, 2710, 1, 147, 278, 5, 402, 4, 10, 6299, 23, 150, 57, 1, 357, 165, 1, 1330, 4, 7]\n"
     ]
    }
   ],
   "source": [
    "print(max(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10271\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from numpy import array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            513550    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10271)             1037371   \n",
      "=================================================================\n",
      "Total params: 1,701,821\n",
      "Trainable params: 1,701,821\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "210194/210194 [==============================] - 358s 2ms/step - loss: 6.1361 - accuracy: 0.0955\n",
      "Epoch 2/100\n",
      "210194/210194 [==============================] - 407s 2ms/step - loss: 5.6557 - accuracy: 0.1327\n",
      "Epoch 3/100\n",
      "210194/210194 [==============================] - 410s 2ms/step - loss: 5.4332 - accuracy: 0.1513\n",
      "Epoch 4/100\n",
      "210194/210194 [==============================] - 354s 2ms/step - loss: 5.2913 - accuracy: 0.1610\n",
      "Epoch 5/100\n",
      "210194/210194 [==============================] - 347s 2ms/step - loss: 5.1750 - accuracy: 0.1670\n",
      "Epoch 6/100\n",
      "210194/210194 [==============================] - 337s 2ms/step - loss: 5.0697 - accuracy: 0.1738\n",
      "Epoch 7/100\n",
      "210194/210194 [==============================] - 344s 2ms/step - loss: 4.9726 - accuracy: 0.1793\n",
      "Epoch 8/100\n",
      "210194/210194 [==============================] - 342s 2ms/step - loss: 4.8860 - accuracy: 0.1827\n",
      "Epoch 9/100\n",
      "210194/210194 [==============================] - 348s 2ms/step - loss: 4.8107 - accuracy: 0.1862\n",
      "Epoch 10/100\n",
      "210194/210194 [==============================] - 349s 2ms/step - loss: 4.7394 - accuracy: 0.1894\n",
      "Epoch 11/100\n",
      "210194/210194 [==============================] - 345s 2ms/step - loss: 4.6735 - accuracy: 0.1916\n",
      "Epoch 12/100\n",
      "210194/210194 [==============================] - 329s 2ms/step - loss: 4.6143 - accuracy: 0.1950\n",
      "Epoch 13/100\n",
      "210194/210194 [==============================] - 325s 2ms/step - loss: 4.5592 - accuracy: 0.1971\n",
      "Epoch 14/100\n",
      "210194/210194 [==============================] - 333s 2ms/step - loss: 4.5076 - accuracy: 0.1998\n",
      "Epoch 15/100\n",
      "210194/210194 [==============================] - 341s 2ms/step - loss: 4.4608 - accuracy: 0.2023\n",
      "Epoch 16/100\n",
      "210194/210194 [==============================] - 340s 2ms/step - loss: 4.4167 - accuracy: 0.2050\n",
      "Epoch 17/100\n",
      "210194/210194 [==============================] - 335s 2ms/step - loss: 4.3761 - accuracy: 0.2070\n",
      "Epoch 18/100\n",
      "210194/210194 [==============================] - 334s 2ms/step - loss: 4.3380 - accuracy: 0.2088\n",
      "Epoch 19/100\n",
      "210194/210194 [==============================] - 344s 2ms/step - loss: 4.3024 - accuracy: 0.2119\n",
      "Epoch 20/100\n",
      "210194/210194 [==============================] - 347s 2ms/step - loss: 4.2685 - accuracy: 0.2136\n",
      "Epoch 21/100\n",
      "210194/210194 [==============================] - 343s 2ms/step - loss: 4.2351 - accuracy: 0.2158\n",
      "Epoch 22/100\n",
      "210194/210194 [==============================] - 349s 2ms/step - loss: 4.2052 - accuracy: 0.2176\n",
      "Epoch 23/100\n",
      "210194/210194 [==============================] - 347s 2ms/step - loss: 4.1756 - accuracy: 0.2204\n",
      "Epoch 24/100\n",
      "210194/210194 [==============================] - 350s 2ms/step - loss: 4.1484 - accuracy: 0.2225\n",
      "Epoch 25/100\n",
      "210194/210194 [==============================] - 352s 2ms/step - loss: 4.1219 - accuracy: 0.2243\n",
      "Epoch 26/100\n",
      "210194/210194 [==============================] - 356s 2ms/step - loss: 4.0962 - accuracy: 0.2269\n",
      "Epoch 27/100\n",
      "210194/210194 [==============================] - 353s 2ms/step - loss: 4.0709 - accuracy: 0.2289\n",
      "Epoch 28/100\n",
      "210194/210194 [==============================] - 350s 2ms/step - loss: 4.0462 - accuracy: 0.2313\n",
      "Epoch 29/100\n",
      "210194/210194 [==============================] - 354s 2ms/step - loss: 4.0239 - accuracy: 0.2323\n",
      "Epoch 30/100\n",
      "210194/210194 [==============================] - 361s 2ms/step - loss: 3.9993 - accuracy: 0.2355\n",
      "Epoch 31/100\n",
      "210194/210194 [==============================] - 360s 2ms/step - loss: 3.9762 - accuracy: 0.2370\n",
      "Epoch 32/100\n",
      "210194/210194 [==============================] - 366s 2ms/step - loss: 3.9557 - accuracy: 0.2396\n",
      "Epoch 33/100\n",
      "210194/210194 [==============================] - 463s 2ms/step - loss: 3.9351 - accuracy: 0.2408\n",
      "Epoch 34/100\n",
      "210194/210194 [==============================] - 545s 3ms/step - loss: 3.9119 - accuracy: 0.2436\n",
      "Epoch 35/100\n",
      "210194/210194 [==============================] - 419s 2ms/step - loss: 3.8916 - accuracy: 0.2457\n",
      "Epoch 36/100\n",
      "210194/210194 [==============================] - 420s 2ms/step - loss: 3.8716 - accuracy: 0.2480\n",
      "Epoch 37/100\n",
      "210194/210194 [==============================] - 420s 2ms/step - loss: 3.8503 - accuracy: 0.2492\n",
      "Epoch 38/100\n",
      "210194/210194 [==============================] - 421s 2ms/step - loss: 3.8324 - accuracy: 0.2514\n",
      "Epoch 39/100\n",
      "210194/210194 [==============================] - 423s 2ms/step - loss: 3.8122 - accuracy: 0.2542\n",
      "Epoch 40/100\n",
      "210194/210194 [==============================] - 400s 2ms/step - loss: 3.7944 - accuracy: 0.2568\n",
      "Epoch 41/100\n",
      "210194/210194 [==============================] - 399s 2ms/step - loss: 3.7744 - accuracy: 0.2585\n",
      "Epoch 42/100\n",
      "210194/210194 [==============================] - 393s 2ms/step - loss: 3.7563 - accuracy: 0.2606\n",
      "Epoch 43/100\n",
      "210194/210194 [==============================] - 380s 2ms/step - loss: 3.7384 - accuracy: 0.2631\n",
      "Epoch 44/100\n",
      "210194/210194 [==============================] - 414s 2ms/step - loss: 3.7207 - accuracy: 0.2646\n",
      "Epoch 45/100\n",
      "210194/210194 [==============================] - 415s 2ms/step - loss: 3.7026 - accuracy: 0.2665\n",
      "Epoch 46/100\n",
      "210194/210194 [==============================] - 411s 2ms/step - loss: 3.6834 - accuracy: 0.2689\n",
      "Epoch 47/100\n",
      "210194/210194 [==============================] - 384s 2ms/step - loss: 3.6670 - accuracy: 0.2714\n",
      "Epoch 48/100\n",
      "210194/210194 [==============================] - 357s 2ms/step - loss: 3.6498 - accuracy: 0.2731\n",
      "Epoch 49/100\n",
      "210194/210194 [==============================] - 359s 2ms/step - loss: 3.6316 - accuracy: 0.2758\n",
      "Epoch 50/100\n",
      "210194/210194 [==============================] - 365s 2ms/step - loss: 3.6161 - accuracy: 0.2780\n",
      "Epoch 51/100\n",
      "210194/210194 [==============================] - 366s 2ms/step - loss: 3.6001 - accuracy: 0.2796\n",
      "Epoch 52/100\n",
      "210194/210194 [==============================] - 366s 2ms/step - loss: 3.5825 - accuracy: 0.2808\n",
      "Epoch 53/100\n",
      "210194/210194 [==============================] - 364s 2ms/step - loss: 3.5664 - accuracy: 0.2832\n",
      "Epoch 54/100\n",
      "210194/210194 [==============================] - 357s 2ms/step - loss: 3.5496 - accuracy: 0.2846\n",
      "Epoch 55/100\n",
      "210194/210194 [==============================] - 357s 2ms/step - loss: 3.5336 - accuracy: 0.2880\n",
      "Epoch 56/100\n",
      "210194/210194 [==============================] - 357s 2ms/step - loss: 3.5169 - accuracy: 0.2899\n",
      "Epoch 57/100\n",
      "210194/210194 [==============================] - 357s 2ms/step - loss: 3.5024 - accuracy: 0.2919\n",
      "Epoch 58/100\n",
      "210194/210194 [==============================] - 365s 2ms/step - loss: 3.4870 - accuracy: 0.2939\n",
      "Epoch 59/100\n",
      "210194/210194 [==============================] - 360s 2ms/step - loss: 3.4706 - accuracy: 0.2953\n",
      "Epoch 60/100\n",
      "210194/210194 [==============================] - 361s 2ms/step - loss: 3.4559 - accuracy: 0.2978\n",
      "Epoch 61/100\n",
      "210194/210194 [==============================] - 608s 3ms/step - loss: 3.4398 - accuracy: 0.3001\n",
      "Epoch 62/100\n",
      "210194/210194 [==============================] - 546s 3ms/step - loss: 3.4244 - accuracy: 0.3014\n",
      "Epoch 63/100\n",
      "210194/210194 [==============================] - 365s 2ms/step - loss: 3.4083 - accuracy: 0.3042\n",
      "Epoch 64/100\n",
      "210194/210194 [==============================] - 364s 2ms/step - loss: 3.3960 - accuracy: 0.3057\n",
      "Epoch 65/100\n",
      "210194/210194 [==============================] - 360s 2ms/step - loss: 3.3783 - accuracy: 0.3084\n",
      "Epoch 66/100\n",
      "210194/210194 [==============================] - 356s 2ms/step - loss: 3.3664 - accuracy: 0.3100\n",
      "Epoch 67/100\n",
      "210194/210194 [==============================] - 350s 2ms/step - loss: 3.3499 - accuracy: 0.3131\n",
      "Epoch 68/100\n",
      "210194/210194 [==============================] - 355s 2ms/step - loss: 3.3360 - accuracy: 0.3147\n",
      "Epoch 69/100\n",
      "210194/210194 [==============================] - 362s 2ms/step - loss: 3.3206 - accuracy: 0.3161\n",
      "Epoch 70/100\n",
      "210194/210194 [==============================] - 360s 2ms/step - loss: 3.3067 - accuracy: 0.3182\n",
      "Epoch 71/100\n",
      "210194/210194 [==============================] - 365s 2ms/step - loss: 3.2931 - accuracy: 0.3202\n",
      "Epoch 72/100\n",
      "210194/210194 [==============================] - 367s 2ms/step - loss: 3.2792 - accuracy: 0.3223\n",
      "Epoch 73/100\n",
      "210194/210194 [==============================] - 359s 2ms/step - loss: 3.2645 - accuracy: 0.3246\n",
      "Epoch 74/100\n",
      "210194/210194 [==============================] - 357s 2ms/step - loss: 3.2509 - accuracy: 0.3261\n",
      "Epoch 75/100\n",
      "210194/210194 [==============================] - 357s 2ms/step - loss: 3.2375 - accuracy: 0.3281\n",
      "Epoch 76/100\n",
      "210194/210194 [==============================] - 358s 2ms/step - loss: 3.2244 - accuracy: 0.3301\n",
      "Epoch 77/100\n",
      "210194/210194 [==============================] - 357s 2ms/step - loss: 3.2110 - accuracy: 0.3316\n",
      "Epoch 78/100\n",
      "210194/210194 [==============================] - 357s 2ms/step - loss: 3.1993 - accuracy: 0.3333\n",
      "Epoch 79/100\n",
      "210194/210194 [==============================] - 356s 2ms/step - loss: 3.1836 - accuracy: 0.3349\n",
      "Epoch 80/100\n",
      "210194/210194 [==============================] - 357s 2ms/step - loss: 3.1714 - accuracy: 0.3381\n",
      "Epoch 81/100\n",
      "210194/210194 [==============================] - 362s 2ms/step - loss: 3.1592 - accuracy: 0.3389\n",
      "Epoch 82/100\n",
      "210194/210194 [==============================] - 363s 2ms/step - loss: 3.1450 - accuracy: 0.3420\n",
      "Epoch 83/100\n",
      "210194/210194 [==============================] - 363s 2ms/step - loss: 3.1329 - accuracy: 0.3428\n",
      "Epoch 84/100\n",
      "210194/210194 [==============================] - 365s 2ms/step - loss: 3.1187 - accuracy: 0.3457\n",
      "Epoch 85/100\n",
      "210194/210194 [==============================] - 361s 2ms/step - loss: 3.1057 - accuracy: 0.3465\n",
      "Epoch 86/100\n",
      "210194/210194 [==============================] - 356s 2ms/step - loss: 3.0957 - accuracy: 0.3494\n",
      "Epoch 87/100\n",
      "210194/210194 [==============================] - 356s 2ms/step - loss: 3.0847 - accuracy: 0.3511\n",
      "Epoch 88/100\n",
      "210194/210194 [==============================] - 357s 2ms/step - loss: 3.0716 - accuracy: 0.3521\n",
      "Epoch 89/100\n",
      "210194/210194 [==============================] - 347s 2ms/step - loss: 3.0580 - accuracy: 0.3550\n",
      "Epoch 90/100\n",
      "210194/210194 [==============================] - 345s 2ms/step - loss: 3.0478 - accuracy: 0.3559\n",
      "Epoch 91/100\n",
      "210194/210194 [==============================] - 345s 2ms/step - loss: 3.0367 - accuracy: 0.3582\n",
      "Epoch 92/100\n",
      "210194/210194 [==============================] - 344s 2ms/step - loss: 3.0212 - accuracy: 0.3603\n",
      "Epoch 93/100\n",
      "210194/210194 [==============================] - 344s 2ms/step - loss: 3.0142 - accuracy: 0.3612\n",
      "Epoch 94/100\n",
      "210194/210194 [==============================] - 343s 2ms/step - loss: 3.0003 - accuracy: 0.3634\n",
      "Epoch 95/100\n",
      "210194/210194 [==============================] - 342s 2ms/step - loss: 2.9919 - accuracy: 0.3655\n",
      "Epoch 96/100\n",
      "210194/210194 [==============================] - 354s 2ms/step - loss: 2.9784 - accuracy: 0.3669\n",
      "Epoch 97/100\n",
      "210194/210194 [==============================] - 355s 2ms/step - loss: 2.9665 - accuracy: 0.3695\n",
      "Epoch 98/100\n",
      "210194/210194 [==============================] - 352s 2ms/step - loss: 2.9585 - accuracy: 0.3708\n",
      "Epoch 99/100\n",
      "210194/210194 [==============================] - 350s 2ms/step - loss: 2.9494 - accuracy: 0.3712\n",
      "Epoch 100/100\n",
      "210194/210194 [==============================] - 360s 2ms/step - loss: 2.9395 - accuracy: 0.3728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x10f515cf8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# compile model\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "#model.fit(X, y, batch_size=128, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exile or by the hand of the executioner the lesson which he thus receives makes him cautious he leaves politics represses his pride and saves pence avarice is enthroned as his bosoms lord and assumes the style of the great king the rational and spirited elements sit humbly on the ground\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1604, 13, 23, 1, 305, 2, 1, 3957, 1, 1813, 11, 8, 214, 1715, 324, 45, 5280, 8, 2105, 455, 7088, 25, 1832, 3, 4178, 7089, 2032, 5, 5281, 17, 25, 5282, 1267, 3, 2396, 1, 473, 2, 1, 103, 652, 1, 786, 3, 1211, 578, 1992, 5283, 57, 1, 725]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "print(encoded)\n",
    "#encoded = encoded[0 : 50]\n",
    "#print(len(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# predict probabilities for each word\n",
    "yhat = model.predict_classes(encoded, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_word = ''\n",
    "for word, index in tokenizer.word_index.items():\n",
    "\tif index == yhat:\n",
    "\t\tout_word = word\n",
    "\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "into man and in the same way the words of the soul and the afterthought of the ground into the valley in which they are born of prose to the state to be a shadow of the soul and the unjust of the soul and the afterthought of the amatory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
